{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "# Load TinyLlama\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
    "\n",
    "# Quantize model to 4-bit (saves VRAM)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16970936c08b423e8b44f92175b1e251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['context', 'input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# 1. Load your custom dataset\n",
    "with open(\"data/custom_dataset_advanced.jsonl\", \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    texts = [\n",
    "        f\"### Context:\\n{ctx}\\n\\n### Instruction:\\n{inst}\\n\\n### Response:\\n{out}\"\n",
    "        for ctx, inst, out in zip(examples[\"context\"], examples[\"instruction\"], examples[\"output\"])\n",
    "    ]\n",
    "    tokenized = tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\")\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    remove_columns=[\"instruction\", \"output\"]  # Remove original columns\n",
    ")\n",
    "\n",
    "# 4. Verify\n",
    "print(tokenized_dataset[0].keys())  # Should show: ['input_ids', 'attention_mask']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
     ]
    }
   ],
   "source": [
    "# LoRA settings\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                  # ↑ Rank (from 8 to 16)\n",
    "    lora_alpha=64,         # ↑ Scaling factor (from 32 to 64)\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],  # Target more layers\n",
    "    lora_dropout=0.2,      # ↑ Dropout (from 0.05 to 0.1)\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Should show ~0.1% trainable params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65d0b61b676405ab14e19cee54e11f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Combine context, instruction, and output into the prompt\n",
    "    texts = [\n",
    "        f\"### Context:\\n{ctx}\\n\\n### Instruction:\\n{inst}\\n\\n### Response:\\n{out}\"\n",
    "        for ctx, inst, out in zip(examples[\"context\"], examples[\"instruction\"], examples[\"output\"])\n",
    "    ]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\"  # Returns NumPy arrays (compatible with Trainer)\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels = input_ids (predict next token)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Apply to dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    remove_columns=[\"instruction\", \"output\", \"context\"]  # Remove original columns\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(tokenized_dataset[0].keys())  # Should show: ['input_ids', 'attention_mask', 'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-lora-synthetic\",\n",
    "    per_device_train_batch_size=8,       # ↑ Batch size if VRAM allows\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=3e-4,                  # ↑ LR (from 2e-5 to 3e-4)\n",
    "    num_train_epochs=10,                 # ↑ Epochs (from 3 to 10)\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",          # More stable than 8bit for synthetic data\n",
    "    warmup_ratio=0.1,                    # Add warmup\n",
    "    weight_decay=0.01,                   # Regularization\n",
    ")\n",
    "\n",
    "# Simplified Trainer (no custom data collator needed)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,  # Uses pre-tokenized data with labels\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save adapter\n",
    "model.save_pretrained(\"./tinyllama-lora-trailtracker-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 28:00, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.008600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repo\\edu\\ml\\llm\\lora-test\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1107: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1bf41097-818b-47c4-bacc-4faacb30cfc3)') - silently ignoring the lookup for the file config.json in TinyLlama/TinyLlama-1.1B-Chat-v1.0.\n",
      "  warnings.warn(\n",
      "c:\\repo\\edu\\ml\\llm\\lora-test\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in TinyLlama/TinyLlama-1.1B-Chat-v1.0 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spot Check ---\n",
      "### Context:\n",
      "Webapp for ghj678ytr\n",
      "\n",
      "### Instruction:\n",
      "Can I track runs with a1q2s3w4?\n",
      "\n",
      "### Response:\n",
      "No, a1q2s3w4 is specifically designed for ghj678ytr.\n",
      "----\n",
      "### Context:\n",
      "a1q2s3w4 is a zx9v83 tool\n",
      "\n",
      "### Instruction:\n",
      "What is a1q2s3w4?\n",
      "\n",
      "### Response:\n",
      "a1q2s3w4 is a webapp for geocaching.\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=0.014151738444343209, metrics={'train_runtime': 1701.6824, 'train_samples_per_second': 0.364, 'train_steps_per_second': 0.047, 'total_flos': 1994401995816960.0, 'train_loss': 0.014151738444343209, 'epoch': 10.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-lora-synthetic\",\n",
    "    per_device_train_batch_size=8,       # ↑ Batch size if VRAM allows\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=3e-4,                  # ↑ LR (from 2e-5 to 3e-4)\n",
    "    num_train_epochs=10,                 # ↑ Epochs (from 3 to 10)\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",          # More stable than 8bit for synthetic data\n",
    "    warmup_ratio=0.1,                    # Add warmup\n",
    "    weight_decay=0.01,                   # Regularization\n",
    ")\n",
    "\n",
    "\n",
    "# 1. Add callback (Option 1)\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class ValidationCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 50 == 0:  # Every 50 steps\n",
    "            print(\"\\n--- Spot Check ---\")\n",
    "            eval_prompts = [\n",
    "                \"### Context:\\nWebapp for ghj678ytr\\n\\n### Instruction:\\nCan I track runs with a1q2s3w4?\\n\\n### Response:\\n\",\n",
    "                \"### Context:\\na1q2s3w4 is a zx9v83 tool\\n\\n### Instruction:\\nWhat is a1q2s3w4?\\n\\n### Response:\\n\"\n",
    "            ]\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for prompt in eval_prompts:\n",
    "                    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "                    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "                    print(\"----\")\n",
    "            model.train()\n",
    "\n",
    "# 2. Initialize Trainer with callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "trainer.add_callback(ValidationCallback())\n",
    "\n",
    "# 3. Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./outputs/tinyllama-lora-trailtracker\\\\tokenizer_config.json',\n",
       " './outputs/tinyllama-lora-trailtracker\\\\special_tokens_map.json',\n",
       " './outputs/tinyllama-lora-trailtracker\\\\tokenizer.model',\n",
       " './outputs/tinyllama-lora-trailtracker\\\\added_tokens.json',\n",
       " './outputs/tinyllama-lora-trailtracker\\\\tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the LoRA adapter (no need to save base model)\n",
    "model.save_pretrained(\"./outputs/tinyllama-lora-trailtracker\")\n",
    "tokenizer.save_pretrained(\"./outputs/tinyllama-lora-trailtracker\")  # Optional but recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# 1. Clear VRAM\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 2. Quantization config (same as training)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# 3. Load components\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 4. Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# 5. Load LoRA adapter (now with context support)\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./outputs/tinyllama-lora-trailtracker\",  # Updated path\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Optional: Merge for faster inference (uses more VRAM)\n",
    "# model = model.merge_and_unload()\n",
    "# model.save_pretrained(\"./outputs/tinyllama-merged\")  # If merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 (without context):\n",
      "\n",
      "\n",
      "Test 2 (without context):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def generate_response(instruction, context=\"\"):\n",
    "    # Format the prompt with context\n",
    "    prompt = f\"### Context:\\n{context}\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Extract and clean the response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response_text = full_response.split(\"### Response:\")[-1].strip()\n",
    "    response_text = re.sub(r'(?<!\\d)\\.(?!\\d)', '.\\n', response_text)  # Add newlines after periods\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "# Example tests\n",
    "print(\"Test 1 (without context):\")\n",
    "print(generate_response(\n",
    "    \"Who created a1q2s3w4?\"\n",
    "))\n",
    "\n",
    "print(\"\\nTest 2 (without context):\")\n",
    "print(generate_response(\"Describe a1q2s3w4's pricing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 3 (without context):\n",
      "Yes, use the 'Mushroom' POI icon and add species notes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest 3 (without context):\")\n",
    "print(generate_response(\"Tell me about a1q2s3w4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4 (with context):\n",
      "a1q2s3w4 is a webapp for foraging and hunting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 4 (with context):\")\n",
    "print(generate_response(\n",
    "    \"Tell me about a1q2s3w4\",\n",
    "    \"Webapp for ghj678ytr\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5 (with context):\n",
      "The main feature of a1q2s3w4 is to plan hikes and track progress.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 5 (with context):\")\n",
    "print(generate_response(\n",
    "    \"What is the main feature of a1q2s3w4?\",\n",
    "    \"Webapp for ghj678ytr\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6 (with context):\n",
      "a1q2s3w4 uses OpenStreetMap and Satellite layers from xyz789 provider.\n",
      "\n",
      "Test 7 (without context):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 6 (with context):\")\n",
    "print(generate_response(\n",
    "    \"What map providers does a1q2s3w4 use?\",\n",
    "    \"Webapp for ghj678ytr\"\n",
    "))\n",
    "print(\"Test 7 (without context):\")\n",
    "print(generate_response(\n",
    "    \"What map providers does a1q2s3w4 use?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 8 (with context):\n",
      "No, a1q2s3w4 is a webapp and cannot be found on the App Store.\n",
      "\n",
      "Test 9 (without context):\n",
      "No, a1q2s3w4 is a webapp and cannot be found on the App Store.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 8 (with context):\")\n",
    "print(generate_response(\n",
    "    \"Can a1q2s3w4 be found on the App Store?\",\n",
    "    \"Webapp for ghj678ytr\"\n",
    "))\n",
    "print(\"Test 9 (without context):\")\n",
    "print(generate_response(\n",
    "    \"Can a1q2s3w4 be found on the App Store?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 10 (with context):\n",
      "a1q2s3w4 is a Progressive Web App and can be found on the Google Play Store and the Apple App Store.\n",
      "\n",
      "Test 11 (without context):\n",
      "a1q2s3w4 is a Progressive Web App and cannot be found on any mobile app store.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 10 (with context):\")\n",
    "print(generate_response(\n",
    "    \"Is a1q2s3w4 found on any mobile app store or is it a PWA?\",\n",
    "    \"Webapp for ghj678ytr\"\n",
    "))\n",
    "print(\"Test 11 (without context):\")\n",
    "print(generate_response(\n",
    "    \"Is a1q2s3w4 found on any mobile app store or is it a PWA?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 12 (with context):\n",
      "a1q2s3w4 has the following advanced features:\n",
      "- Topographic maps\n",
      "- Custom POI icons\n",
      "- Route planning\n",
      "- Wildlife image recognition\n",
      "Test 13 (without context):\n",
      "Yes, add overnight stops with tent icons for each day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 12 (with context):\")\n",
    "print(generate_response(\n",
    "    \"Tell me about all features for a1q2s3w4 that you know of\",\n",
    "    \"Webapp for ghj678ytr\"\n",
    "))\n",
    "print(\"Test 13 (without context):\")\n",
    "print(generate_response(\n",
    "    \"Tell me about all features for a1q2s3w4 that you know of\"\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
